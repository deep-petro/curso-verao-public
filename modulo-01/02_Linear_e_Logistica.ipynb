{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-Linear e Logistica.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBvUmcHxonaS"
      },
      "source": [
        "# Aprendizado Profundo - UFMG\n",
        "\n",
        "## Preâmbulo\n",
        "\n",
        "O código abaixo consiste dos imports comuns. Além do mais, configuramos as imagens para ficar de um tamanho aceitável e criamos algumas funções auxiliares. No geral, você pode ignorar a próxima célula."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-iRUIgyouKm"
      },
      "source": [
        "# -*- coding: utf8\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams['figure.figsize']  = (18, 10)\n",
        "plt.rcParams['axes.labelsize']  = 20\n",
        "plt.rcParams['axes.titlesize']  = 20\n",
        "plt.rcParams['legend.fontsize'] = 20\n",
        "plt.rcParams['xtick.labelsize'] = 20\n",
        "plt.rcParams['ytick.labelsize'] = 20\n",
        "plt.rcParams['lines.linewidth'] = 4\n",
        "torch.set_printoptions(precision=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5J4r1Joonac"
      },
      "source": [
        "plt.ion()\n",
        "\n",
        "plt.style.use('seaborn-colorblind')\n",
        "plt.rcParams['figure.figsize']  = (12, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hxSo7nonag"
      },
      "source": [
        "Para testar o resultado dos seus algoritmos vamos usar o módulo testing do numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKAN6JuJonah"
      },
      "source": [
        "from numpy.testing import assert_equal\n",
        "from numpy.testing import assert_almost_equal\n",
        "from numpy.testing import assert_array_almost_equal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVe1G2wonak"
      },
      "source": [
        "## Regressão Linear e Logística from Scratch\n",
        "\n",
        "Para brincar um pouco mais com essa diferenciação automatica, presente em frameworks como pytorch e mxnet, nesta aula vamos implementar a regressão linear e logística do zero. Vamos fazer duas versões de cada:\n",
        "\n",
        "1. Derivando na mão, não é complicado.\n",
        "2. Derivando com autograd de pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waN0vx9fonal"
      },
      "source": [
        "## Conjunto de Problemas 1: Mais Derivadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK0g7SKdonam"
      },
      "source": [
        "Antes de entrar na regressão, vamos brincar um pouco de derivadas dentro de funções. Dado dois números `x` e `y`, implemente a função `log_exp` que retorna:\n",
        "\n",
        "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsgyDlAHpBdZ"
      },
      "source": [
        "def log_exp(x, y):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnoHouBPonap"
      },
      "source": [
        "1. Abaixo vamos testar o seu código com algumas entradas simples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fd-sln-w2Op"
      },
      "source": [
        "x, y = torch.tensor([2.0]), torch.tensor([3.0])\n",
        "z = log_exp(x, y)\n",
        "z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQNEAUNoonau"
      },
      "source": [
        "# Teste. Não apague\n",
        "assert_almost_equal(1.31326175, z.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdhvPzMxonax"
      },
      "source": [
        "2. A função a seguir computa $\\partial z/\\partial x$ e $\\partial z/\\partial y$ usando `autograd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHBQrZD7qsYN"
      },
      "source": [
        "# O argumento funcao_forward é uma função python. Será a sua log_exp.\n",
        "# A ideia aqui é deixar claro a ideia de forward e backward propagation, depois\n",
        "# de avaliar a função chamamos backward e temos as derivadas.\n",
        "def grad(funcao_forward, x, y):\n",
        "    x.requires_grad_(True)\n",
        "    y.requires_grad_(True)\n",
        "    z = funcao_forward(x, y)\n",
        "    z.backward()\n",
        "    return x.grad, y.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WFpkBMvona2"
      },
      "source": [
        "Testando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMVfDIKQrAJL"
      },
      "source": [
        "x, y = torch.tensor([2.0], dtype = torch.double) ,torch.tensor([3.0], dtype = torch.double)\n",
        "dx, dy = grad(log_exp, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIkTfFH0snPD"
      },
      "source": [
        "assert_almost_equal(-0.7310586, dx.numpy())\n",
        "assert_almost_equal(0.7310586, dy.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oNpz1wIona9"
      },
      "source": [
        "4. Agora teste com números maiores, algum problema?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qEsHY5VsuH_"
      },
      "source": [
        "x, y = torch.tensor([400.0]).double() ,torch.tensor([800.0]).double()\n",
        "grad(log_exp, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGUiPmJLonbB"
      },
      "source": [
        "5. Pense um pouco sobre o motivo do erro acima. Usando as propriedade de logaritmos, é possível fazer uma função mais estável. Abaixo segue a implementação da mesma. O problema aqui é que o exponencial \"explode\" quando x ou y são muito grandes. Este [link](http://www.wolframalpha.com/input/?i=log[e%5Ex+%2F+[e%5Ex+%2B+e%5Ey]]) pode ajudar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnj8B4EyttvL"
      },
      "source": [
        "x, y = torch.tensor([400.0], dtype = torch.double) ,torch.tensor([800.0], dtype = torch.double)\n",
        "def stable_log_exp(x, y):\n",
        "    return torch.log(torch.exp(y-x))\n",
        "\n",
        "dx, dy = grad(stable_log_exp, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9QIVgN1onbF"
      },
      "source": [
        "stable_log_exp(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVWzzA4-t9J2"
      },
      "source": [
        "# Teste. Não apague\n",
        "assert_equal(-1, dx.numpy())\n",
        "assert_equal(1, dy.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esV2XR7jonbL"
      },
      "source": [
        "O exemplo acima mostra um pouco de problemas de estabilidade númerica. Às vezes é melhor usar versões alternativas de funções. Isto vai ocorrer quando você ver vários nans na sua frente :-) Claro, estamos assumindo que existe uma outra função equivalente que é mais estável para o computador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qI91TX7onbM"
      },
      "source": [
        "## Conjunto de Problemas 2: Regressão Linear\n",
        "\n",
        "Agora, vamos explorar uma regressão linear. Embora não vamos fazer uso da logexp acima, a ideia de derivar parcialmente dentro de funções pode nos ajudar. Lembrando da regressão linear, inicialmente temos um conjunto observações representadas como tuplas $(\\mathbf{x}_i, y_i)$. Aqui, $\\mathbf{x}_i$ é um vetor de atributos. Vamo forçar $\\mathbf{x}_{i0} = 1$, capturando assim o intercepto. Além do mais, $\\mathbf{x}_{ij}$ quando $j\\neq 0$, são os outros atributos de entrada. $y_i$ um valor real representando uma resposta. Nossa regressão visa capturar:\n",
        "\n",
        "$$y_i = 1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots + \\theta_k x_{if}$$\n",
        "\n",
        "Lembrando da regressão linear multivariada, podemos representar as equações como uma multiplicação de uma matriz com um vetor\n",
        "\n",
        "![](./figs/linear.png)\n",
        "\n",
        "7. Crie uma função de previsao. A mesma recebe uma matrix $\\mathbf{X}$ e um vetor de parâmetros $\\theta$. Sua função deve retornar um vetor de previsões para cada linha de $\\mathbf{X}$. Não use nenhum laço!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HveutorucYMJ"
      },
      "source": [
        "def previsao(X, theta):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNRX8E0MonbP"
      },
      "source": [
        "**Erros quadrados**. Para aprender os parâmetros ótimos da regressão linear, precisamos fazer uso de um modelo de erros quadrados. Em particular nosso objetico é aprender os parâmetros que minimizam a função:\n",
        "\n",
        "$$L(\\mathbf{\\theta}) = n^{-1} \\sum_i ({\\hat{y}_i - y_i})^2$$\n",
        "\n",
        "Onde $\\hat{y}_i$ é uma previsão (vêm da sua função python acima). $y_i$ é o valor real dos dados.\n",
        "\n",
        "8. Implemente uma função para a média dos os erros quadrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS7y5Wb2cusy"
      },
      "source": [
        "def media_erros_quadrados(X, theta, y):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqJCucconbT"
      },
      "source": [
        "9. Agora, crie uma função que deriva o erro acima. A função deve usar o autograd do pytorch (função backward). Lembre-se que temos um vetor de parâmetros $\\theta$. Por sorte, você pode fazer derivadas de tais vetores também."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ao1G8uSc8Bp"
      },
      "source": [
        "def derivada_torch(X, theta, y): #Lembre que o theta passado devera ser uma variavel com requires_grad == True\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnWTcJubonba"
      },
      "source": [
        "10. A versão a seguir não usa autograd. As derivadas são implementadas do zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBpOTZjPdLrq"
      },
      "source": [
        "def derivada_navera(X, theta, y):\n",
        "    return ((torch.matmul(X, theta) - y) * X.T).mean(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj5LkKLzonbf"
      },
      "source": [
        "11. Por fim, otimize sua função usando o algoritmo de gradiente descendente abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-9buFPGfVPG"
      },
      "source": [
        "def gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001, max_iter=10000):\n",
        "    '''\n",
        "    Executa Gradiente Descendente. Aqui:\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    d_fun : é uma função de derivadas\n",
        "    loss_fun : é uma função de perda\n",
        "    X : é um vetor de fatores explanatórios.\n",
        "        Copie seu código de intercepto da primeira aula.\n",
        "        para adicionar o intercepto em X.\n",
        "    y : é a resposta\n",
        "    lambda : é a taxa de aprendizad\n",
        "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
        "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
        "              paramos depois de max_iter iterações.\n",
        "    '''\n",
        "    theta = torch.randn(X.shape[1])\n",
        "    theta.requires_grad_(True)\n",
        "    \n",
        "    print('Iter {}; theta = '.format(0), theta)\n",
        "    \n",
        "    old_err_sq = np.inf\n",
        "    i = 0\n",
        "    while True:\n",
        "        # Computar as derivadas\n",
        "                \n",
        "        theta.requires_grad_(True)  #Necessario pois ao final, theta recebe o theta novo, que não tem requires_grad == True\n",
        "        grad = d_fun(X,theta,y)\n",
        "\n",
        "        # Atualizar\n",
        "        with torch.no_grad(): \n",
        "            theta_novo = theta - lambda_ * grad\n",
        "        \n",
        "        #Parar quando o erro convergir\n",
        "        err_sq = loss_fun(X, theta, y)\n",
        "        if torch.abs(old_err_sq - err_sq) <= tol:\n",
        "            break\n",
        "        \n",
        "         #Atualizar parâmetros e erro\n",
        "        theta = theta_novo\n",
        "        old_err_sq = err_sq\n",
        "        \n",
        "        # Informação de debug\n",
        "        print('Iter {}; theta = '.format(i+1), theta)\n",
        "        i += 1\n",
        "        if i == max_iter:\n",
        "            break\n",
        "        \n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVWvK5mffuqd"
      },
      "source": [
        "# Para testes, não apague!!!\n",
        "X = torch.zeros(1000, 2)\n",
        "X[:, 0] = 1.0\n",
        "X[:, 1] = torch.randn(1000)\n",
        "\n",
        "theta_0_real = 7.0\n",
        "theta_1_real = 9.0\n",
        "y = theta_0_real + theta_1_real * X[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxE4o8ALonbm"
      },
      "source": [
        "# Testando com derivada autograd. Sua função deve retornar algo perto de 7 e 9\n",
        "theta = gd(derivada_torch, media_erros_quadrados, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YZwD4GZonbo"
      },
      "source": [
        "theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DuRmmjonbr"
      },
      "source": [
        "# Testando com derivada manual. Sua função deve retornar algo perto de 7 e 9\n",
        "theta = gd(derivada_navera, media_erros_quadrados, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6SbuwB7onbt"
      },
      "source": [
        "theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQY5_lJvonbv"
      },
      "source": [
        "12. Altere a função de Gradiente Descendente para funcionar com minibatches. Em outras palavras, não compute o erro usando todos os dados de X. Use um `slice` de tamanho do minibatch. Uma ideia é seguir o pseudocódigo abaixo.\n",
        "\n",
        "```python\n",
        "index = np.arange(len(X))\n",
        "while True:\n",
        "    minib = np.random.choice(index, minibatchsize) # aqui estou usando numpy para selection minibatch elementos\n",
        "    X_batch = X[minib]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOyf2F26onbv",
        "scrolled": true
      },
      "source": [
        "# Exemplo abaixo\n",
        "index = np.arange(len(X))\n",
        "mb = np.random.choice(index, 50)\n",
        "print(mb)\n",
        "X[mb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbssWxyCWBNY"
      },
      "source": [
        "def minibatch_gd(d_fun, loss_fun, X, y, lambda_=0.01, tol=0.00001,\n",
        "                 max_iter=10000, batch_size=10):\n",
        "    '''\n",
        "    Executa Gradiente Descendente. Aqui:\n",
        "    \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    d_fun : é uma função de derivadas\n",
        "    loss_fun : é uma função de perda\n",
        "    X : é um vetor de fatores explanatórios.\n",
        "        Copie seu código de intercepto da primeira aula.\n",
        "        para adicionar o intercepto em X.\n",
        "    y : é a resposta\n",
        "    lambda : é a taxa de aprendizad\n",
        "    tol : é a tolerância, define quando o algoritmo vai parar.\n",
        "    max_ter : é a segunda forma de parada, mesmo sem convergir\n",
        "              paramos depois de max_iter iterações.\n",
        "    batch_size : tamanho do batch\n",
        "    '''\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gzuUW4onb3"
      },
      "source": [
        "# Sua função deve retornar algo perto de 7 e 9\n",
        "minibatch_gd(derivada_torch, media_erros_quadrados, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khoTY_BZonb7"
      },
      "source": [
        "## Conjunto de Problemas 3: Logistic from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puVaL8Kuonb8"
      },
      "source": [
        "12. Repita o mesmo processo para a Logística. Lembrando que a mesma tem a seguinte forma:\n",
        "\n",
        "$$f(x_i) = \\frac{1}{1 + e^{-(1 + \\theta_1 x_{i1} + \\theta_2 x_{i2} + \\cdots \\theta_k x_{ik})}}$$\n",
        "\n",
        "Implemente a função logística."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gMYzjfDWBNh"
      },
      "source": [
        "def logistic(X, theta):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-AsqWqHWBNk"
      },
      "source": [
        "# testes, não apague!\n",
        "X_teste = torch.randn(1000, 20000)\n",
        "theta = torch.randn(20000)\n",
        "y_hat_teste = logistic(X_teste, theta)\n",
        "assert_equal(True, (y_hat_teste >= 0).numpy().all())\n",
        "assert_equal(True, (y_hat_teste <= 1).numpy().all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcOzTSYqoncA"
      },
      "source": [
        "Usando a logística acima implemente uma função logistica_prever que retorna 0 ou 1. Use o limiar dado na função. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IzJAoDYoncB"
      },
      "source": [
        "def logistica_prever(X, theta, limiar=0.5):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeNGI7IFWBNs"
      },
      "source": [
        "# testes, não apague!\n",
        "X_teste = torch.randn(1000, 20000)\n",
        "theta = torch.randn(20000)\n",
        "y_hat_teste = logistica_prever(X_teste, theta)\n",
        "for yi in y_hat_teste.numpy():\n",
        "    assert(yi in {0, 1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34WQX2AloncI"
      },
      "source": [
        "Agora, implemente uma função de entropia cruzada da logística. A mesma, é proporcional ao inverso da verossimilhança. Para entender a derivação entre as duas faça uso dos [Slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing). \n",
        "\n",
        "Sendo:\n",
        "\n",
        "$$ll(x_i,y_i~|~\\theta) = y_i \\log f(x_i\\theta) + (1-y_i) \\log (1-f(x_i\\theta))$$\n",
        "\n",
        "A verossimilhança para uma observação. A entropia cruzada é a media da negação do termo para todos os exemplos:\n",
        "\n",
        "$$L(\\theta) = -n^{-1}\\sum_i \\big((1-y_i)\\log (1-f_{\\theta}(x_i)) + y_i\\log (f_{\\theta}(x_i))\\big)$$\n",
        "\n",
        "`dica: use torch.clamp(logistic(X, theta), min = 0.001, max = 0.999)`. A função remove os 0 e 1s da logistic, evitando assim o valor log(0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5XRS87bWBNw"
      },
      "source": [
        "def cross_entropy_mean(X, theta, y):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_fPXj0oncM"
      },
      "source": [
        "# testes, não apague!\n",
        "from sklearn import datasets\n",
        "state = np.random.seed(20190187)\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2)\n",
        "X = torch.tensor(X).double()\n",
        "y = torch.tensor(y).double()\n",
        "\n",
        "for _ in range(100):\n",
        "    theta = torch.randn(2,1).double()\n",
        "    assert(cross_entropy_mean(X, theta, y) >= 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1E_BSkhoncO"
      },
      "source": [
        "Agora implemente a derivada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJqaZncSWBN2"
      },
      "source": [
        "def derivada_torch_logit(X, theta, y):\n",
        "    # implemente\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4kELMG4oncS"
      },
      "source": [
        "A partir daqui basta executar código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZP6mlFIoncT"
      },
      "source": [
        "plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], s=80, edgecolors='k')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlRLH5sMoncW"
      },
      "source": [
        "13. Assim como foi feito na primeira aula, abaixo testamos o seu código com um toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mY_Dfj8oncW"
      },
      "source": [
        "# Use essa função antes de executar o GD\n",
        "def add_intercept(X):\n",
        "    Xn = torch.zeros(X.shape[0], X.shape[1] + 1).double()\n",
        "    Xn[:, 0]  = 1.0\n",
        "    Xn[:, 1:] = X\n",
        "    return Xn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT4OCJwYoncb"
      },
      "source": [
        "Xn = add_intercept(X)\n",
        "theta = minibatch_gd(derivada_torch_logit, cross_entropy_mean, Xn, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Iaopm_onch"
      },
      "source": [
        "y_p = logistica_prever(Xn, theta)\n",
        "print(y == y_p.int().double())\n",
        "print((y == y_p.int().double()).double().mean())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}